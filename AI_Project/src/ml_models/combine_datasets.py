"""
Combine original features with synthetic data features
Creates an extended dataset for better ML training

This uses a simplified approach: since synthetic data comes from the same
source files, we'll assign them average feature values from similar pairs
in the original dataset, which is reasonable for testing scalability.
"""

import pandas as pd
import json
import numpy as np
from pathlib import Path


def main():
    """Combine original and synthetic datasets"""
    
    # Paths
    original_features = Path("src/feature_selection/artifacts/features.csv")
    synthetic_dir = Path("data/augmented")
    metadata_file = synthetic_dir / "pairs_metadata.json"
    output_features = Path("src/feature_selection/artifacts/features_extended.csv")
    
    print("\n" + "="*80)
    print("COMBINING ORIGINAL AND SYNTHETIC DATASETS")
    print("="*80 + "\n")
    
    # Load original features
    print("Loading original features...")
    df_original = pd.read_csv(original_features)
    print(f"Original dataset: {len(df_original)} pairs")
    
    # Load synthetic metadata
    print("Loading synthetic data metadata...")
    with open(metadata_file, 'r') as f:
        synthetic_pairs = json.load(f)
    print(f"Synthetic pairs: {len(synthetic_pairs)}")
    
    # Create synthetic features by sampling from original
    # For plagiarized pairs: use mean of plagiarized pairs from original
    # For non-plagiarized pairs: use mean of non-plagiarized pairs from original
    print("\nGenerating features for synthetic pairs...")
    
    # Get mean features for each class from original data
    feature_cols = [col for col in df_original.columns if col not in ['file1', 'file2', 'label']]
    
    plagiarized_mean = df_original[df_original['label'] == 1][feature_cols].mean()
    non_plagiarized_mean = df_original[df_original['label'] == 0][feature_cols].mean()
    
    # Add some random noise to make them more realistic (Â±10%)
    synthetic_rows = []
    for i, pair in enumerate(synthetic_pairs):
        if pair['is_plagiarized'] == 1:
            # Use plagiarized mean with noise
            features = plagiarized_mean + plagiarized_mean * np.random.normal(0, 0.1, size=len(plagiarized_mean))
        else:
            # Use non-plagiarized mean with noise
            features = non_plagiarized_mean + non_plagiarized_mean * np.random.normal(0, 0.1, size=len(non_plagiarized_mean))
        
        # Clip values to reasonable ranges
        features = features.clip(lower=0)
        
        # Create row
        row = {
            'file1': pair['file1'],
            'file2': pair['file2'],
            'label': pair['is_plagiarized']
        }
        row.update(features.to_dict())
        synthetic_rows.append(row)
    
    df_synthetic = pd.DataFrame(synthetic_rows)
    print(f"Generated features for {len(synthetic_rows)} synthetic pairs")
    
    # Ensure column order matches original
    df_synthetic = df_synthetic[df_original.columns]
    
    # Combine datasets
    df_combined = pd.concat([df_original, df_synthetic], ignore_index=True)
    
    # Save combined dataset
    output_features.parent.mkdir(parents=True, exist_ok=True)
    df_combined.to_csv(output_features, index=False)
    
    print("\n" + "="*80)
    print("DATASET COMBINATION COMPLETE")
    print("="*80)
    print(f"Original dataset: {len(df_original)} pairs")
    print(f"Synthetic dataset: {len(df_synthetic)} pairs")
    print(f"Combined dataset: {len(df_combined)} pairs")
    print(f"\nClass distribution:")
    print(df_combined['label'].value_counts())
    print(f"\nSaved to: {output_features}")
    print("="*80 + "\n")
    
    print("To train models with extended dataset:")
    print(f"  python src/ml_models/train_models.py --features {output_features}")


if __name__ == "__main__":
    main()
